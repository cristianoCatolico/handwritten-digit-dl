{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwriting Digit Recognition\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "- Have a model trained with MNIST dataset\n",
    "- Have an evaluation of the model\n",
    "- Application that use the model trained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup parameters\n",
    "\n",
    "In device I set cpu since I do not have gpu. Batch size usually should be power of 2. I started changing with 2 and then changing this value, until I found that for 50 I obtain a good accuracy.  Learning rate I started with 0.1, however I modified and got better result with 0.001. According to the majority of epochs used in MNIST dataset, the number of epochs recommended is 6. I obtain good result with 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%pycodestyle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 50\n",
    "NUM_CATEGORIES = 10\n",
    "NUM_EPOCHS = 5\n",
    "DEVICE = torch.device('cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Since MNIST is a public dataset well-known, it is included in the framework. In my case, I choose pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%pycodestyle\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='',\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='',\n",
    "    train=False,\n",
    "    transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition and Training\n",
    "### Definition of Convolutional Neural Network\n",
    "I choose a CNN because between deep learning models the one used for images is CNN. And really comparing with other models CNN obtain almost 99% of accuracy. The CNN is composed of two Convolutional Layers and one fully connected layer. Also we have consider optimization of the neural network using BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%pycodestyle \n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, numCategories=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "          nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=16,\n",
    "            kernel_size=5,\n",
    "            stride=1,\n",
    "            padding=2),\n",
    "          nn.BatchNorm2d(num_features=16),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "         nn.Conv2d(\n",
    "          in_channels=16,\n",
    "          out_channels=32,\n",
    "          kernel_size=5,\n",
    "          stride=1,\n",
    "          padding=2),\n",
    "         nn.BatchNorm2d(num_features=32),\n",
    "         nn.ReLU(),\n",
    "         nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fc = nn.Linear(7*7*32, numCategories)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "cnn_mnist = ConvNet(NUM_CATEGORIES).to(DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%pycodestyle\n",
    "def fit(model, train_loader):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    numSteps = len(train_loader)\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if ((i+1) % 100) == 0 :\n",
    "                print(f'Epoch: [{epoch+1}:{NUM_EPOCHS}]; batchStep: [{i+1}/{numSteps}]; Loss: {loss.item()}')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1:5]; batchStep: [100/1200]; Loss: 0.11209528893232346\n",
      "Epoch: [1:5]; batchStep: [200/1200]; Loss: 0.23784486949443817\n",
      "Epoch: [1:5]; batchStep: [300/1200]; Loss: 0.04622058570384979\n",
      "Epoch: [1:5]; batchStep: [400/1200]; Loss: 0.06823167204856873\n",
      "Epoch: [1:5]; batchStep: [500/1200]; Loss: 0.20222480595111847\n",
      "Epoch: [1:5]; batchStep: [600/1200]; Loss: 0.0715143233537674\n",
      "Epoch: [1:5]; batchStep: [700/1200]; Loss: 0.054954953491687775\n",
      "Epoch: [1:5]; batchStep: [800/1200]; Loss: 0.05416314676403999\n",
      "Epoch: [1:5]; batchStep: [900/1200]; Loss: 0.027382677420973778\n",
      "Epoch: [1:5]; batchStep: [1000/1200]; Loss: 0.03680666908621788\n",
      "Epoch: [1:5]; batchStep: [1100/1200]; Loss: 0.09735927730798721\n",
      "Epoch: [1:5]; batchStep: [1200/1200]; Loss: 0.0403018519282341\n",
      "Epoch: [2:5]; batchStep: [100/1200]; Loss: 0.026827611029148102\n",
      "Epoch: [2:5]; batchStep: [200/1200]; Loss: 0.058852873742580414\n",
      "Epoch: [2:5]; batchStep: [300/1200]; Loss: 0.03133438900113106\n",
      "Epoch: [2:5]; batchStep: [400/1200]; Loss: 0.1148897111415863\n",
      "Epoch: [2:5]; batchStep: [500/1200]; Loss: 0.0062552825547754765\n",
      "Epoch: [2:5]; batchStep: [600/1200]; Loss: 0.1149781122803688\n",
      "Epoch: [2:5]; batchStep: [700/1200]; Loss: 0.06429286301136017\n",
      "Epoch: [2:5]; batchStep: [800/1200]; Loss: 0.024292202666401863\n",
      "Epoch: [2:5]; batchStep: [900/1200]; Loss: 0.05871950089931488\n",
      "Epoch: [2:5]; batchStep: [1000/1200]; Loss: 0.03624735772609711\n",
      "Epoch: [2:5]; batchStep: [1100/1200]; Loss: 0.06832237541675568\n",
      "Epoch: [2:5]; batchStep: [1200/1200]; Loss: 0.0026105279102921486\n",
      "Epoch: [3:5]; batchStep: [100/1200]; Loss: 0.030389226973056793\n",
      "Epoch: [3:5]; batchStep: [200/1200]; Loss: 0.056883521378040314\n",
      "Epoch: [3:5]; batchStep: [300/1200]; Loss: 0.0052786897867918015\n",
      "Epoch: [3:5]; batchStep: [400/1200]; Loss: 0.0013900124467909336\n",
      "Epoch: [3:5]; batchStep: [500/1200]; Loss: 0.03083730861544609\n",
      "Epoch: [3:5]; batchStep: [600/1200]; Loss: 0.09088509529829025\n",
      "Epoch: [3:5]; batchStep: [700/1200]; Loss: 0.016761958599090576\n",
      "Epoch: [3:5]; batchStep: [800/1200]; Loss: 0.026223652064800262\n",
      "Epoch: [3:5]; batchStep: [900/1200]; Loss: 0.011126209050416946\n",
      "Epoch: [3:5]; batchStep: [1000/1200]; Loss: 0.014416531659662724\n",
      "Epoch: [3:5]; batchStep: [1100/1200]; Loss: 0.010309392586350441\n",
      "Epoch: [3:5]; batchStep: [1200/1200]; Loss: 0.007258012890815735\n",
      "Epoch: [4:5]; batchStep: [100/1200]; Loss: 0.001407322590239346\n",
      "Epoch: [4:5]; batchStep: [200/1200]; Loss: 0.0027220677584409714\n",
      "Epoch: [4:5]; batchStep: [300/1200]; Loss: 0.02458532154560089\n",
      "Epoch: [4:5]; batchStep: [400/1200]; Loss: 0.003832833142951131\n",
      "Epoch: [4:5]; batchStep: [500/1200]; Loss: 0.06857869774103165\n",
      "Epoch: [4:5]; batchStep: [600/1200]; Loss: 0.06217331439256668\n",
      "Epoch: [4:5]; batchStep: [700/1200]; Loss: 0.0038853390142321587\n",
      "Epoch: [4:5]; batchStep: [800/1200]; Loss: 0.003831547684967518\n",
      "Epoch: [4:5]; batchStep: [900/1200]; Loss: 0.0027272701263427734\n",
      "Epoch: [4:5]; batchStep: [1000/1200]; Loss: 0.041335057467222214\n",
      "Epoch: [4:5]; batchStep: [1100/1200]; Loss: 0.0019185477867722511\n",
      "Epoch: [4:5]; batchStep: [1200/1200]; Loss: 0.0010314963292330503\n",
      "Epoch: [5:5]; batchStep: [100/1200]; Loss: 0.004526165314018726\n",
      "Epoch: [5:5]; batchStep: [200/1200]; Loss: 0.04200317710638046\n",
      "Epoch: [5:5]; batchStep: [300/1200]; Loss: 0.00048307693214155734\n",
      "Epoch: [5:5]; batchStep: [400/1200]; Loss: 0.008281109854578972\n",
      "Epoch: [5:5]; batchStep: [500/1200]; Loss: 0.034004442393779755\n",
      "Epoch: [5:5]; batchStep: [600/1200]; Loss: 0.0008842370007187128\n",
      "Epoch: [5:5]; batchStep: [700/1200]; Loss: 0.005048717372119427\n",
      "Epoch: [5:5]; batchStep: [800/1200]; Loss: 0.04321762174367905\n",
      "Epoch: [5:5]; batchStep: [900/1200]; Loss: 0.004222604911774397\n",
      "Epoch: [5:5]; batchStep: [1000/1200]; Loss: 0.028819095343351364\n",
      "Epoch: [5:5]; batchStep: [1100/1200]; Loss: 0.012802548706531525\n",
      "Epoch: [5:5]; batchStep: [1200/1200]; Loss: 0.1137923076748848\n",
      "ConvNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=1568, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "fit(cnn_mnist, train_loader=train_loader)\n",
    "print(cnn_mnist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:1: W391 blank line at end of file\n"
     ]
    }
   ],
   "source": [
    "#%%pycodestyle\n",
    "def evaluate(model):\n",
    "    correct = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for test_imgs, test_labels in test_loader:\n",
    "        y_true.extend(test_labels.numpy())\n",
    "        outputs = model(test_imgs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        correct += (predicted == test_labels).sum()\n",
    "    print(f'Test accuracy: {float(correct) / (len(test_loader)*BATCH_SIZE)}')\n",
    "    return (y_true, y_pred)\n",
    "\n",
    "\n",
    "test_labels, test_predictions = evaluate(cnn_mnist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:1: W391 blank line at end of file\n"
     ]
    }
   ],
   "source": [
    "#%%pycodestyle\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cf_matrix = confusion_matrix(test_labels, test_predictions)\n",
    "print(classification_report(test_labels, test_predictions))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(cf_matrix, annot=True, cbar=None, cmap=\"YlGnBu\", fmt=\"d\")\n",
    "plt.title(\"Confusion Matrix\", fontsize=17)\n",
    "plt.ylabel(\"True Class\", fontsize=13)\n",
    "plt.xlabel(\"Predicted Class\", fontsize=13)\n",
    "plt.show()\n",
    "\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "print(\"Accuracy   :\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cnn_mnist, 'cnn_trained.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application to do the Inference\n",
    "I have developed using Tkinter an application that allows to do a handwritten digit. Then I have consumed the model showing the number that is obtained from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80:1: W391 blank line at end of file\n"
     ]
    }
   ],
   "source": [
    "#%%pycodestyle\n",
    "import torch\n",
    "from tkinter import *\n",
    "import tkinter as tk\n",
    "from PIL import ImageGrab, Image\n",
    "import numpy as np\n",
    "model = torch.load('cnn_trained.pt')\n",
    "\n",
    "\n",
    "def predict_digit(img):\n",
    "    img = img.resize((28, 28))\n",
    "    img = img.convert('L')\n",
    "    transform = transforms.ToTensor()\n",
    "    img_valid = transform(img)\n",
    "    img_valid = img_valid.to(DEVICE)\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    output = model(img_valid.unsqueeze(0))\n",
    "    _, prediction = torch.max(output, 1)\n",
    "    return prediction.item()\n",
    "\n",
    "\n",
    "class App(tk.Tk):\n",
    "    def __init__(self):\n",
    "        tk.Tk.__init__(self)\n",
    "\n",
    "        self.x = self.y = 0\n",
    "        self.winfo_toplevel().title(\"Handwriting Digit Recognition\")\n",
    "        # Creating elements\n",
    "        self.canvas = tk.Canvas(\n",
    "            self,\n",
    "            width=300,\n",
    "            height=300,\n",
    "            bg=\"white\",\n",
    "            cursor=\"cross\")\n",
    "        self.label = tk.Label(self, text=\"Thinking..\", font=(\"Helvetica\", 48))\n",
    "        self.classify_btn = tk.Button(\n",
    "            self,\n",
    "            text=\"Recognise\",\n",
    "            command=self.classify_handwriting)\n",
    "        self.button_clear = tk.Button(\n",
    "            self,\n",
    "            text=\"Clear\",\n",
    "            command=self.clear_all)\n",
    "\n",
    "        # Grid structure\n",
    "        self.canvas.grid(row=0, column=0, pady=2, sticky=W, )\n",
    "        self.label.grid(row=0, column=1, pady=2, padx=2)\n",
    "        self.classify_btn.grid(row=1, column=1, pady=2, padx=2)\n",
    "        self.button_clear.grid(row=1, column=0, pady=2)\n",
    "\n",
    "        self.canvas.bind(\"<B1-Motion>\", self.draw_lines)\n",
    "\n",
    "    def clear_all(self):\n",
    "        self.canvas.delete(\"all\")\n",
    "\n",
    "    def classify_handwriting(self):\n",
    "        x, y = (self.canvas.winfo_rootx(), self.canvas.winfo_rooty())\n",
    "        width, height = (self.canvas.winfo_width(), self.canvas.winfo_height())\n",
    "        rect = (x, y, x+width, y+height)\n",
    "        im = ImageGrab.grab(rect)\n",
    "\n",
    "        digit = predict_digit(im)\n",
    "        self.label.configure(text=str(digit))\n",
    "\n",
    "    def draw_lines(self, event):\n",
    "        self.x = event.x\n",
    "        self.y = event.y\n",
    "        r = 8\n",
    "        self.canvas.create_oval(\n",
    "            self.x-r,\n",
    "            self.y - r,\n",
    "            self.x + r,\n",
    "            self.y + r,\n",
    "            fill='black')\n",
    "\n",
    "\n",
    "app = App()\n",
    "mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
